%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami p rzepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[magisterska,en]{pracamgr}

% Dane magistranta:
\autor{Jakub Kopeć}{417354}

% Dane magistrantów:
%\autor{Autor Zerowy}{342007}
%\autori{Autor Pierwszy}{342013}
%\autorii{Drugi Autor-Z-Rzędu}{231023}
%\autoriii{Trzeci z Autorów}{777321}
%\autoriv{Autor nr Cztery}{432145}
%\autorv{Autor nr Pięć}{342011}

\title{Exploration of cooperation-enabling solutions in HPC}
\titlepl{Stworzenie portalu służącego do transferu dużych zbiorów danych}

% \tytulang{Multidimensional Feature Selection and High Performance ParalleX}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computational Engineering}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{\bfseries Marek Michalewicz, Ph.D.\\
Interdisciplinary Centre for Mathematical\\and Computational Modelling\\
%\bfseries Piotr Bała, professor, Ph.D.\\
%Interdisciplinary Centre for Mathematical\\and Computational Modelling
  }

% miesiąc i~rok:
\date{Warsaw, March 2020}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
% 11.2 Statystyka\\ 
113000 - Informatics, Computer Science \\ 
%11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{
  Computing methodologies $\sim$ Massively parallel algorithms\\
  Computing methodologies $\sim$ Distributed algorithms \\
  Computing methodologies $\sim$ Feature selection
  }

% Słowa kluczowe:
\keywords{do uzupelnienia}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]
\usepackage{subfig}
\usepackage{float}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{arrows,calc,decorations.markings,math,arrows.meta}
\usetikzlibrary{external}
\usepackage{afterpage}
\usepackage[normalem]{ulem}
\usepackage{placeins}
\interfootnotelinepenalty=10000


% koniec definicji

\begin{document}
\maketitle
\thispagestyle{empty}


\chapter{Introduction}
\section{Scope of the thesis}
The author has been faced with the task of exploring the cooperation-enabling solutions that may be used to boost research efficiency by facilitating the collaboration between scientists. On the next stages of the work author was to choose one of the solution that in his feeling was the most crucial one and implement it in the Interdisciplinary Centre for Mathematical and Computational Modelling of University of Warsaw.
\section{SAGE2}
\subsection{SAGE2 overview}
 First, the author came up with the idea of making use of the ICM's video wall in order to promote cooperation between researcher's at geographically distributed science facilities - that was the reason of the concept of implementing the SAGE2 software emergence. SAGE stands for Scalable Amplified Group Environment and it is Node.js-based (JavaScript) software that facilitates use of large video-walls that are intended to be used by multiple users at a time. It works as the server's software - all the resource-demanding operations are handled by the server, so the end-user do not need to posses powerful workstation in order to use a video-wall. The special script run on the machine that operate the displays creates a server that provide two services. The first one is Google Chrome-based interface that displays the working environment on the video-wall. The second one is web-accessible portal that allows authenticated users (each SAGE2 session could be password-protected) to control content displayed on the video-wall. When user connects to the server he is presented a simplified schema of the video-wall that shows how the display space is arranged and an intuitive interface that allows to control the contents of the display. \cite{SAGE2_developers} SAGE2 provides user with the modules that may be displayed on the screen. The essential ones, like web browser, google maps, notepad etc. are already implemented by the creators of the SAGE2, but there is special appstore where developers publish their own modules designated to support another software. If one would like to create his own module the developer's guide for making such module is available at SAGE2 project homepage.
 \subsection{SAGE2 implementation at ICM UW}
 During the installation of the SAGE2 in Technology Center of Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) author encountered few problems concerning the network configuration as SAGE2 server required public IP address and DMZ network what could create some complications if the network design had not been adapted to such use. The ICM technicians bypassed these issues with ports forwarding, but they also noticed a security weakness induced by such solution - open unsecured port pose threat of unauthorized access to the network. In order to eliminate such possibility the access to the port used by SAGE2 was secured with username/password authentication. After the installation author prepared a presentation for ICM's staff about the functionality and instructions on how to use SAGE2 software. The presentation was followed by a discussion on possible appliances of SAGE2 as a tool for cooperation between research facilities. The most repeated remark was that this software allows only cooperation between two SAGE2 sites and that there is practically no support for users that are not present in front of the video-wall. Another issue that was pointed out by the audience was the fact that API for making own SAGE2 module is rather fixed on JavaScript and it would not be easy to create such module for an application that was not designed in advance to support such functionality. The last but not least was the matter of security of SAGE2. The audience noticed that there is no clear declaration about the decryption used by SAGE2 and that SAGE2 protocols could not be sufficiently secure to be used in projects that require confidentiality.
 \subsection{SAGE2 summary}
 To sum up, the SAGE2 could act as platform for conducting collaborative research, but in present-day form it may be used locally as middleware for large resolution screens rather than for remote collaborative work. Even though the creators of SAGE2 successfully conducted remote collaborative work session \cite{SAGE2_developers}, in the author's opinion preparing such sessions require significant effort to establish reliable connection between two SAGE2 sites that would be justified only in case of long cooperation between two institutions where multiple SAGE2 sessions would bring noticeable boost in cooperation. Moreover the issues mentioned in the previous paragraph should be addressed beforehand.s On the other side - SAGE2 is perfect tool to facilitate the collaborative effort in case when all users are physically present in front of the video-wall. Perfect example of such use is StickySchedule app that was intended to be launched on SAGE2 site and it's purpose is to ease and precipitate the conferences scheduling \cite{SAGE2_Conference_Scheduling}. In the end, it unfortunately turned out that the scope of this task is by far too narrow to act as master thesis material, thereby the author got back to the exploration of other solutions.
\section{Data Transfer Portal (DTP)}
\subsection{DTP overview}
The second idea, that author was guided onto, was to create from scratch a web portal that would mask IT's expertise-demanding part of big data moving aspects from the end user and simplify such task as much as possible. The draft name for the project was "Data Transfer Portal" (abr. DTP). The main motivation behind the project was fact that software that is used in HPC applications to move large amount of data is rather unfriendly and unintuitive for the user that is not IT-technician responsible for data transfer. Not only is the use of such software complicated, but it is also necessary to test the connection properties between source and destination in order to optimise the transfer. The DTP is intended to handle all this operations and provide the end-user with simple web interface that is easy to use and do not require IT expertise. On the beginning author committed some time to learn how to use django framework with python \cite{djangotutorial} as it seemed that project would require creating a web portal at some point. Nevertheless, when author started to think on how DTP should look like he encountered a problem trying to answer the question "How DTP server will know that the user that require data transfer is really who he claim that he is and if he is allowed to transfer that data (permissions control)?". At that point author completely focused on research on user authentication and authorization methods. The main issue was the fact that assumedly users would not be the members of one organization and each user should be a member of at least two different parties (one source and one destination).
\subsection{Authentication methods}
During his research the author found three authentication that may be used to solve the problem presented in the previous subsection\cite{auth_meth}:
\begin{itemize}
 \item basic authentication - user and password (may be encrypted)
 \item SAML - Security Assertion Markup Language\cite{saml2}
 \item Oauth2.0\cite{OAuth2} with OpenID\cite{OpenID}
\end{itemize}
\subsubsection{SAML - Security Assertion Markup Language}
SAML is a XML-based standard created and maintained by OASIS (Organization for the Advancement of Structured Information Standards). It's main purpose is to describe how the security information could be exchanged on-line between two separate parties. It is based on the exchange of standardised messages , called SAML assertions, that are created according to the standard's syntax and rules. The framework's assumption is to provide components that could be used in many configuration to meet the user's requirements. Moreover, the SAML specification includes profiles that are predefined to satisfy the most common use-cases.\cite{saml2}
\subsubsection{OpenID Connect}
OpenID Connect is the authentication standard used on top of the OAuth2.0 authorization protocol. In the previous versions OpenID and OAuth were separate standards. OpenId's purpose was to verify the identity of the user based on the authentication that is performed by OpenID provider\cite{OpenID_old}. OAuth 2.0 protocol was responsible for verification of the user permissions to the requested assets\cite{OAuth2} while OpenID just ensures the service provider that the user is in control of some identifier (e.g. the gmail account) and there was no way of determining if the user name or any other data are valid and real. It is possible to create OpenID provider on one's own \cite{own_id_server} and use it to issue conduct completely valid authentication to the service provider using OpenID. In the newest version of the standard - OpenID Connect - these two protocols were connected and now OpenID provides not only the user authentication but it also enables the user authorization.\cite{OpenID}
\subsubsection{Difference between SAML and OpenID}
Before the implementation of the OpenID Connect there was a significant difference between SAML and OpenID. First of all the OpenID2.0 is the authentication protocol while SAML provides the authorization and the authentication as well. Secondly, SAML authorization was based on the thrust relationship and on the beforehand arrangements between partners. The service provider trusted that the identity provider is or was able to authenticate the user real identity. The example of such scenario are e-identity services available on government's and local authorities' sites. When a citizen is to fulfill administrative matters online he may use his bank account to log in to authorities' portal. In this case the user's bank is the identity provider and the government's site is a service provider that trusts that bank's employee verified the user's identity (for example - checked his ID card or passport) when he opened the bank account. SAML standardised the messages used in authentication (and, on the next stages, in authorization) process. As for OpenID such authentication was not possible as identity provider could not guarantee truthfulness of the user's data). After the introduction of the OpenID Connect where authorization was joined with authentication the difference in functions between those two standards blurred. Nevertheless the implementation of OpenID Connect is simpler than the implementation of SAML protocol so OpenID is used in simpler web applications while the SAML is used in large federations (e.g. university or enterprise federations) as it has already been widely adopted in existing federations and is more mature standard than OpenID. On the other hand SAML is restricted to browser use, so in the case of application or device usage OpenID is the obvious choice.\cite{Ubisec_whitepaper}
\subsection{DTP summary}
At this point the author decided that Data Transfer Portal project is unsuitable subject for this thesis due to the fact that in the initial phase it would require significant amount of rather administrative and legal efforts - preparing appropriate agreements, settling the scope and the fields of cooperation etc. Moreover, in the first place it would be necessary to find and encourage appropriate institutions to take part in the project. The further phases of realization would obviously contain the practical aspect of preparing appropriate infrastructure and creating software that would be appropriate for the scope of this thesis. Nevertheless, taking into account the fact that mentioned administrative matters, that are outside of technical aspect of the problem, would require significant resources and would probably last a few months (or even years) the author decided that he had to find another solution to implement as material for his master thesis. Fortunately during the research of information on DTP the author realised that data transmission is not a trivial problem, especially the transfer of large amount of files between computing centers may pose significant factor that influence the collaboration efforts between researchers.
\chapter{Data transfer and computer networks overview}
\section{Short research on the amount of data created worldwide and global Internet traffic}
As an introduction to the research on the history and the development of data transmission protocol the author decided to look for the information how much data is created nowadays by humanity and how that amount has changed over the history. According to the academic research conducted in 2007 all the information humanity was able to store by 1986 had the capacity of 2.6 optimally compressed exabytes. In the article authors also describe the methodology that they use to estimate the amount of data and the assumptions that they introduced during their research. The storage capacity estimated by the authors of that study grew exponentially through the years: 15.8 EB in 1993, 54.5 EB in 2000, to overwhelming 200 exabytes in 2007 (when the study was conducted). \cite{Storage_study1} The information on the global storage capacity is not easy to estimate as well it is not effortless to find it. One of the sources of such information (also mentioned in \cite{Storage_study1}) is International Data Corporation (IDC) - it is private company that offer market intelligence, advisory services information technology, telecommunications, and consumer technology markets. \cite{IDC_about} IDC prepared recent report on world's storage capacity, but due to the company's policy it is not freely available. Nevertheless in one of their press releases IDC passed two numbers that may be interesting: they estimated that current world's storage capacity is 6.8 ZB and they forecast that this number will increase to 8.9 ZB in 2024. \cite{IDC_press} The fig. 1 was prepared basing on the data presented in this paragraph and one may easily notice that the world storage capacity is now rising as rapidly as never before.
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{storage.jpg}
\caption{Global storage capacity basing on \cite{Storage_study1} and \cite{IDC_press}}
\label{fig:storage}
\end{figure}
\FloatBarrier
The next aspect that author decided to check was the rate of the internet growth. Again it was surprisingly hard to find any accurate information on that matter, but the author came across Cisco's Annual Internet Reports where the technological giant include estimation of average global Internet traffic. In one entry on Cisco's official blog the member of the team responsible for the Annual Internet Reports aggregate all their historical data on global Internet traffic for the years 1984-2014. \cite{Cisco_blog} For the next years and the forecast up to 2022 year it was necessary to went through Cisco's Annual Internet Reports from years 2016-2019 that was published on Cisco's website. \cite{Cisco_VNI} The author aggregated all the results from the reports and presented them on the fig. \ref{fig:traffic}.
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{traffic.jpg}
\caption{Global Internet traffic in years 1984-2017 with forecast up to 2022}
\label{fig:traffic}
\end{figure}
\FloatBarrier
One studying fig. \ref{fig:storage} and fig. \ref{fig:traffic} may easily recognise the exponential growth of the depicted numbers. It is obvious that the growth of Internet traffic and global storage capacity is strictly connected with the new technology development. Over time humanity invented new techniques to transfer and store more and more data.
\section{The beginnings of the computer networking}
If one was to find the beginnings of computer's networking he would have go back to year 1957 when the Soviet Union (USSR - Union of Soviet Socialist Republics) won the space race during the Cold War and launched Sputnik - the first space satellite. As a result of failure the United States' Department of Defense founded Advanced Research Projects Agency that was responsible for execution of innovative research as well as for providing innovative tools that empower the dynamic science development.\cite{Internet_History_article} One of the tools that were used for science that time were computers that created new possibilities for various researchers. It was noticed at once that computer resources, which were scarce and expensive, need to be shared between multiple scientific projects and researchers. To address this problem computer timesharing was introduced - one computer resources were allocated to multiple users - when some users were idle the others would use the computer. L. Kleinrock noticed that such system could be used to share communication links in order to build efficient communication network. He came with the idea of splitting the messages into smaller parts and to send them independently. It was called packet switching and was essential in building reliable networks used in data transmission. Two other researchers - Paul Baran (RAND Corporation) and Donald Davies (National Physical Laboratory (UK)) came up with the same idea independently. In 1962 J.C.R. Licklinder was appointed as the first director of the Information Processing Techniques Office (IPTO) in ARPA\footnote{The Advanced Research Projects Agency (ARPA) name was changed to Defense Advanced Research Projects Agency (DARPA) in 1971. Nevertheless in 1993 it was renamed back to ARPA, just to be renamed again to DARPA in 1996.} and he was the first one to came up with a vision of "galactic" network of computers where interconnected processing nodes will enable anyone to access data and programs from anywhere. \cite{Internet_History_article} In 1965 ARPA recognized the need to connect their researchers to the few computers that was spread across the United States. Such connection would allow ARPA to share its geologically spread resources in a cost-effective manner. The pursuit to fulfill that need led to the project named ARPANET. It is worth noticing that research on this project was exceptional as it was open to broad community researchers.\cite{an_early_history_of_the_internet} There was a problem on how different computers (different architecture, different operating system etc.) could communicate each other. In 1968 Bolt, Beranek and Newman (BBN) came with a project of Interface Message Processor (IMP) that was in fact a separate computer that handled switching and communication functions. There was an IMP at each computer's location which served as an interconnection between the computer and the network. In 1969 the ARPANET was launched - it was the first computer network that connected four computers located at the University of California at Los Angeles (UCLA), the University of California at Santa Barbara (UCSB), the Stanford Research Institute (SRI) in California and the University of Utah. ARPANET utilised Network Control Protocol (NCP) that was the first network protocol - a set of signals that was agreed between parties that was used to establish communication channels and enabled data to be passed between the computers. Over time more computers and smaller subnetworks were connected to the ARPANET, the first international links were created. In 1971 Ray Tomlinson from BBN came up with a minor software that allowed the electronic mail exchange between the computers in the network. After few updates it turned out that users were eager to use this form of communication - after few months emails accounted for the majority of traffic in the ARPANET.  \cite{an_early_history_of_the_internet} In 1974 Vin Cerf and Robert Kahn came up with a new protocols that could supersede NCP as it started to be insufficient.\cite{an_early_history_of_the_internet} The new protocols proposed by Cerf and Kahn were the Transmission Control Protocol that includes set of rules for computer on how to use network (how to establish and break connections) and the Internet Protocol (IP) that determined how individual data packets were routed. These protocols were also implementing open architecture philosophy that was revolutionary at the time - so they were freely available for the whole research community. \cite{Internet_History_article} After few years TCP/IP were implemented widely not only in the ARPANET but also in smaller subnetworks what enabled to exchange data between networks (and therefore between computers) freely without any significant barriers. As a reason the number of computers and links in the global network has risen abruptly over upcoming years. The TCP/IP protocol was updated few times since then, but it is still in use in today's Internet.
Another invention that is worth mentioning in the aspect of global network development is the World Wide Web proposed by Tim Berners-Lee from CERN in 1989. He came up with the idea of building distributed hypermedia server that will allow network users to create and share electronic documents that may comprise of multiple file types such as text files, sounds, pictures etc.. In the upcoming years he created WWW client, Hypertext Transfer Protocol (HTTP) and HyperText Markup Language (HTML) that accelerated significantly development of the global network and created the Internet in the form that we know today.\cite{Internet_History_article}
\section{The Open Systems Interconnection (OSI) Model}
Back in the 1970s the compatibility between different networks and computer systems was a significant challenge. Even though the TCP/IP protocol was already invented and implemented in ARPANET there were no universal standard on how interconnection protocol should look like in general and almost each vendor of computer systems was using it's own protocol. The task to create standard that would satisfy each vendor (different processor architectures/speeds/network interfaces/operating systems/memory etc.) was impracticable, so in 1977 the International Standards Organization (ISO) undertook the task of creating the framework for building open communication standards that was adapted in 1984 as the Open Systems Interconnect (OSI) Model. \cite{OSI_article1} The model was described in the international standard and it also defines what open system mean: "The fact that a system is open does not imply any particular systems implementation, technology or means of interconnection, but refers to the mutual recognition and support of the applicable standards". \cite{OSI_standard} So the ISO did not create new protocol nor described every possible network, but it created the template for creating network protocols that would enable different systems to interconnect each other despite the differences between them. It was accomplished by dividing the communication between the systems into several sub-tasks creating a layered structure where given layer provide services for layer that is above using the services provided by the previous layer. It is not necessary to create separate protocols for each layer, but this reference model is helpful in describing the given protocol's role in the communication process. The OSI model defines 7 layers \cite{OSI_article1, OSI_article2}:
\begin{itemize}
 \item Physical layer
 The physical layer of the protocol describes how to transfer bits of information between devices. The main issue is the medium that is used to transfer information - it could be copper wire (or set of wires), optic fiber or radio waves etc. The physical layer of the protocol also defines how 1's and 0's are represented in the medium (e.g. different voltage levels, transition between voltage levels, light impulses, frequency modulation, how long does impulse last) and how wires, plugs and sockets are built.
 \item Data link layer
 The main data link layer task is to organise bits of information into frames that are in most cases prefixed with preamble pattern of bits that inform interconnected devices that meaningful information will be broadcast in the medium. This layer defines the format of the frames and may implement flow control of data and error detection over physical medium.
 \item Network layer
 The network layer is responsible for passing the data between the source and destination device across the network that may consist of multiple sub-networks. It's main task is to route data between networks via optimal path, but it also may also help to control congestion in sub-nets in order to prevent bottlenecks.
 \item Transport layer
 The transport layer's main task is to provide reliability of message transmission between end users. It splits the data into smaller packets that are sent separately over the network and provides mechanisms that allow verification if the whole message was delivered correctly and flow control of data between the source and destination.
 \item Session Layer
 This layer is responsible for maintaining connection session (established ongoing communication) between interconnected devices and handles the connection recovery and synchronization in case of unexpected disconnection.
 \item Presentation Layer
 The Presentation Layer converts transmitted data into format that is suitable for user's application and reversely - it transform received data (basing on the application's requirements and receiver's system configuration) into format compatible with the application. Data encryption/decryption may be conducted at this stage of communication.
 \item Application Layer
 The last layer specify how end user interface with network services - it allows user or given software to utilise network connection or access remote resources. 
\end{itemize}
\subsection{TCP/IP protocol - the foundation of the Internet}
The protocols mentioned before - Internet Protocol and Transmission Control Protocol - are the fundamentals of today's Internet so they deserve a chapter in this thesis, even though they may be not the fastest and the most effective protocols that may be used in large datasets transfer or in HPC interconnects. The version of TCP/IP protocol that we use to this very day was established in 1980 and was declared as the standard for ARPANET in 1983.\cite{an_early_history_of_the_internet} TCP/IP is used in each communication in the Internet (the Internet, as the name suggest, is just a set of smaller interconnected networks that span around the globe), even though TCP/IP packets may be encapsulated by other protocols packet on route between the source and destination, they are still the basis of the Internet that we know and use today. If one compare TCP/IP with the OSI reference model one may say that TCP is responsible for the tasks of transport layer and IP for the tasks of network layer. One of the reasons of TCP/IP popularity is the fact that these standards are open - so they are available to use for anyone as opposed to proprietary protocols.
\subsection{TCP - Transmission Control Protocol}
TCP is connection-oriented protocol - the whole process of data transmission begins with three-way handshake - side that initiated the connection (client) sends the SYN packet to the server, server responses client with SYN-ACK packet. After receiving SYN-ACK packet the client sends the ACK packet to the server to confirm that SYN-ACK packet has been received - on this stage the connection is established and both ends of the connection are ready to send the data. When TCP obtains the data to send from higher layers it splits them into smaller parts - the packets (packet size depends on system specification and lower layers requirements) - that are send to the destination separately. The packet consist of the header - which contains information on source/destination port, the sequence number, packet size and the checksum, that allows to verify data correctness - and of the actual data that are transmitted by the given packet. The packets may arrive to the receiver out of order or some of them may not arrive at all. The TCP task on the receiver side is to respond for every received packet with ACK packet and to put them in correct order, if sender notice that some sent packets are not responded with ACK it re-transmits them until receiver acknowledge them (or transmission time-out will pass). When the received message is complete and set in correct order the TCP pass the data to higher layers of the receiver. Transferring data using TCP is reliable as the sender is assured that message was received completely and the receiver is able to put the packets in correct order. Once the connection is established data may be sent both ways until it will be terminated with the exchange of appropriate packets (FIN and ACK packets). The main con of using TCP protocol is it's reliability - higher layers' applications do not have to implement any control mechanisms - they may assume that TCP delivered the data (or notify them in case of transmission failure). On the other hand providing reliability in that way may lead to significant overhead in data transmission - first of all, the headers are additional data that need to be transmitted, but more importantly - the packet that arrives the last limits transfer rate - data may not be passed further until the last packet arrives despite the fact that the delivered portion of data may be already processed by an application. \cite{RFC_TCP}
\subsection{IP - Internet Protocol}
The Internet Protocol is responsible for passing the data from the source to the destination. It add to the data passed from higher layers (e.g. TCP packet) it's own header that include sender and the receiver internet addresses (IP numbers). The most challenging task of this protocol is routing process - finding the most optimal path for packet from source to destination. Devices that take part in this process and relay the packets are called routers - it may be dedicated hardware or computer may act as one - that switch incoming packets with the use of routing tables. The routing table is a list maintained by each router that specifies where packets should be passed in order to get to their destination. When the packet arrive to the router it checks if the packet's destination address is in it's routing table - if the destination address is found then the router pass the packet to the path specified by the routing table. But if the destination address is not in router's routing table it passes the packet to it's default gateway that is usually the other router that is higher in hierarchy (have larger/wider routing table) and the process is repeated until the destination address is found. \cite{how_internet_work} The routing tables may be maintained by administrators in smaller networks, but in wide area networks they are maintained and updated autonomously by appropriate algorithms. The routing table may include more than one path for given destination and juxtapose it with metrics that may be updated (e.g. on the base of current path's congestion) and the packets are sent through the most optimal path. As a result the packets that creates one longer message may be sent through different paths as they are sent and routed independently. Due to the fact that nowadays the number of interconnected networks is tremendous (and is still growing) there are multiple paths that could be used for packet routing - that makes the Internet immune to breakdowns. Even if some links or part of networks become corrupted (by natural causes or deliberate actions) the rest of the networks may remain operational as packets may be delivered via alternate paths.
\subsection{UDP - User Datagram Protocol}
UDP was created and defined in 1980 and was intended to work on top of Internet Protocol. It was intended to enable datagram transmission in packet-switched networks. It is simple procedure to exchange messeages between computers in network. UDP is message-oriented it prepares message with data, adds the header with the source port number, the destination port number, the data length and the checksum (that allow receiver to verify data integrity). There are no acknowledgments nor three-way handshake characteristic to TPC - as a result the transmission via UDP is not reliable, but in return the protocol is a lot simpler and the transmission overhead is significantly smaller. UDP may be used in simple application as a basic network interface (with the awareness that the data delivery is not guaranteed) or in scenarios where the real-time data flow is more important than reliable transfer of all packets. Examples of such cases are video/audio streaming, videotalks via the Internet or online computer games - in case of loss a part of the packets the quality of service may be decreased (e.g. lower sound/video quality), but such applications cannot work if sender keeps retransmitting a lost packet instead of providing new packets that are necessary (while the lost packet may be already useless). UDP may be also used when another network layers provide reliability of transfer - if such solution is implemented in a smart way it may provide the reliability similar to TCP, but with lower overhead or more optimal workflow or data processing. \cite{RFC_UDP}
\subsection{Ethernet - an example of protocol concerning data link and physical layers}
The ethernet protocol was designed by Xerox and it was introduced in 1980, after 4 years, in 1984 the transfer of IP datagrams over ethernet network was standardised by Network Working Group in the characteristic form of the Request for Comments. \cite{RFC_ETH} It became a popular standard in LANs and in industrial networks and is still widely used in those fields. The ethernet specifies how multiple hosts in one network share physical medium, the frame format and the physical aspects of the medium (wire/fiber specification, plugs shape and size etc.). Each device that may be connected to the network yields vendor-assigned MAC address (media access control) and it is one of the ethernet's task to enable the hosts to map network's addresses into MAC addresses. It accomplish this task using ARP (Address Resolution Protocol) packets that are broadcast to all host connected to the medium in order to query destination MAC address. The ethernet is also responsible for managing the access to the medium and checking transmitted data integrity (checksums).
\section{File Transfer Protocols and data movers}
The next to mention are the protocols and technologies that allow user to send files containing various data via computer networks. Most of them may be assigned to the highest layers of the OSI reference model as they usually provide services for the end user.
\subsection{FTP - File transfer protocol}
The FTP protocol, that was designed in 1971 at M.I.T. and underwent some changes since then, is still in use up to today. The first implemented versions were running on Network Control Program that was the predecessor of TCP/IP and the current specification was published in October 1985 as RFC 959. The FTP was created in order to encourage users to share files and to use remote computers. Additionally it shielded users from differences between systems among hosts and provided file transfer reliability. The FTP uses two connection to transfer files - the first one is called control connection and is used to exchange FTP commands and messages between the hosts. The second one - data connection - is established using the control connection and is used to transfer the actual data. \cite{RFC_FTP}
\subsection{GridFTP}
Nevertheless FTP is based on TCP and thereby it inherits its limitations resulting from congestion control and reliability mechanisms. For that reason multiple solutions that were to address that issue were created. One of those, worth mentioning, is GridFTP which allows to use multiple TCP streams to overcome that bottleneck. Another issue that was addressed by the authors of GridFTP was the lack of interoperability between various storage systems - usually each vendor created it's own transfer protocol and in order to access data one was to use proprietary libraries or APIs. They proposed the creation of one common protocol that would enable data access and transfer - GridFTP that was extension of the well-established FTP protocol.\cite{GridFTP} Generally such software solutions that are used to transfer data may be termed "data movers".
\subsection{IBM's Aspera Fast And Secure Protocol (FASP)}
Another answer to the dilemma of TCP reliability mechanisms is to use UDP-based protocol with reliability and congestion control implemented in the higher layers, e.g. UDT.\cite{UDT} However this solutions still have its' drawbacks such as unpredictable performance in WAN Internet environment, creating additional redundant traffic or in case of UDT solutions the network may be overflooded with UDP packets what degrades the network performance for other users. Due to that some vendors create proprietary protocols that are intended to overcome this sort of drawback and to provide the best possible transfer performance. IBM's Aspera Fast And Secure Protocol (FASP) may be the example of such proprietary UDP-based solution. First of all, in case of bulk file transmission the packets are not required to come to the destination in the correct order, as opposed to TCP, so the transfer is not blocked by one lost packet being retransmitted. Secondly the FASP send probing packets periodically in order to determine network's properties (delay-based control mechanism) between the source and the destination in order to adapt send rate and aim to efficient use of the available bandwidth. \cite{IBM_Aspera}
\subsection{bbcp}
The next data mover worth mentioning is bbcp - it is TCP-based software designed to conduct point-to-point data transfer over WAN network that allows more efficient link utilisation than regular data coping software (e.g. Linux's secure copy - scp). The bbcp by default uses the TCP's auto-tuning features in order to optimise transfer parameters, nevertheless user is able to freely adjust all the parameters in order to reach desired effect that may be better than in the case of default auto-tuning values. As was in the case of GridFTP, bbcp supports the use of multiple TCP streams what may be beneficial to transfer rate. Other interesting feature of this data mover is real-time copying capability - a file may be still being created by different program while the already existing part of the file is being copied to the destination. \cite{bbcp_homepage}
\subsection{XRootD}
Another software framework that may be used to transfer large amount of files is XRootD whose primary target is to provide access to data that are spread across distributed heterogeneous storage clusters with the lowest possible latency. It is used in CERN and SLAC National Accelerator Laboratory as a tool providing access to data collected during various physical experiments. A communication protocol that is one of the parts of this sophisticated multi-cluster solution may be as well utilised as an effective method to transfer bulk data. \cite{xrootd_homepage, xrootd_paper1, xrootd_paper2}
\subsection{Multicore-Aware Data Transfer Middleware (MDTM)}
One more data mover is the Multicore-Aware Data Transfer Middleware (MDTM) Project which comprise of two parts - MDTM Middleware and mdtmFTP. MDTM Middleware address the problems that arise from the non-uniform memory access model of multicore systems. It's task is to provide I/O locality, i. e. to ensure that data transfer task are assigned to the cores that are placed near the I/O device being used (e.g. network interfaces or storage). Appropriate task scheduling may result in improved I/O throughputs and latencies by reducing the number of inter-node I/O operations. The second part of this solution is mdtmFTP data transfer tool that utilise the benefits of I/O locality provided by MDTM Middleware. It uses multiple TCP streams to transfer data and implements the Large Virtual File Transfer Mechanism to deal with the transfer of lot of small files (LOSF). LOSF transfers pose difficult challenge in data moving field as it is burdened by serious overhead, resulting from the necessity to perform protocol and I/O processing on a per-file basis and from the presence of significant amount of metadata that also need to be processed and transferred. Due to that software and hardware developers are looking for solution to address LOSF issues. In the case of mdtmFTP the Large Virtual File Transfer Mechanism was invented - it creates larger virtual file comprised of the smaller files' metadata that is used to send appropriate data blocks from storage instead of sending all the files separately. \cite{MDTM_homepage}
\subsection{Zettar zx}
The example of proprietary data moving solution is Zettar zx software that operates on application layer of the OSI model and is built on top of TCP. Unfortunately Zettar doesn't reveal the details of how their software works, nevertheless it proves to be highly efficient in data moving challenges \cite{DMC_SC_Asia} and in production environments - it allow to utilise significant part of the available bandwidth and outperforms other solutions. \cite{zettar_product_brief}. It is worth mentioning that Zettar does not advertise itself as a software that allow fast data transmission offhand, but it emphasize that data moving is complex endeavour which requires appropriate infrastructure. The data links are only one part of it, in order to transfer large amount of data efficiently one need computing power and storage resources that are capable of providing the data with the rate that is comparable with the transfer rate. \cite{zettar_white_paper}
\section{Interconnects}
The separate type of networks are interconnects - these are networks that are used in large scale computing systems to provide communication between the nodes of the computer cluster. In High-Performance Computing (HPC) large computers comprised of hundreds of thousands processors are engaged in solving complex computational problems that require appropriate computing power to be solved. In vast majority of computational problems the processors need to exchange data between each other and that is where interconnect comes in. The communication between the processors may make up a significant part of the computational task and that is the reason why the interconnects are required to provide high efficiency and low latency. Approximately a half of the supercomputers on TOP500 list (the list of 500 most powerful computer systems) uses Gigabit Ethernet as a interconnect technology - it is the same protocol that was already mentioned in this thesis, but with appropriate updates. It's popularity in HPC systems probably results from the fact that Ethernet is an open standard and that it is widely adopted technology used in all sorts of computer networks. Approximately one third of supercomputers on the TOP500 list is interconnected using InfiniBand (IB). It's features will be presented in the next chapter as it was used to deal with the problem of transferring large amount of files which is significant part of this thesis. The last interconnect technology that is used on the noticeable part (approximately 10\%) of the systems listed on the TOP500 list is the Intel's Omni-path.\cite{top500_list} Omni-path was promising technology that was sort of combination of InfiniBand with Cray's proprietary Aries interconnect that was acquired by Intel. \cite{intel_leave_omnipath} The Omni-Path 100 series which provided 100 Gb/sec network ports was quite successful. It is worth mentioning that Professor Satoshi Matsuoka, one of the most known and respected HPC system designer and builder, decided to use this interconnect in his Tsubame 3 supercomputer. He observed that Intel Omni-Path consumed less power than InfiniBand and had better thermal stability. Adaptive routing capability of Intel Omni-Path also proved to be successful feature. \cite{tsubame3_omnipath} Intel announced the launch of Omni-Path 200 series that was to provide 200 Gb/sec bandwidth, but unfortunately it never took place. The company unexpectedly stopped to mention that solution and quietly dropped it on 2019. \cite{intel_leave_omnipath} This situation is one of the examples how wrong financial decisions may squander promising technology. The remaining systems from the TOP500 list use custom interconnect, that are designed especially for a given system, or proprietary solution. \cite{top500_list} A latest Cray's solution - Slingshot Interconnect - is a example of proprietary system. The innovative Slingshot's approach is the trial to create Ethernet-compatible solution that join the cons of the Ethernet protocol (from the HPC point of view, e.g. being standard based and operating on commodity hardware) with the features desirable in the HPC networks (such as low latency and scalability). \cite{slingshot_article} Upcoming months will reveal if Cray's approach turn out to be successful. Nevertheless, proprietary interconnect are in decline - Quadrics' QSnet and Myricom's Myrinet are examples of such systems. Proprietary solutions tend to be expensive - for that reason only a narrow group of organisations could afford to use them - what effectively inhibited the expansion of such interconnects.
\section{InfiniBand}
Author decided to devote a significant amount of time on research on InfiniBand (IB) - the interconnect network whose popularity competes with the popularity of the Ethernet on the TOP500 list and is used on 7 out of the first 10 systems listed there (the 3 remaining systems use proprietary/custom solutions). \cite{top500_list} InfiniBand is a standard defining input/output architecture that is used in computer networks communication - especially to interconnect nodes in HPC clusters, servers and storage systems in data centers. The InfiniBand fabric is marked by it's low-latency and high-bandwidth - the features that are essential for HPC systems. At the moment InfiniBand reaches 200Gb/s bandwidth and is planned to match 1.2Tb/s in the near future.\cite{ibta_about_page} But the most important feature of IB is it's message oriented service that uses stack bypass technique to reduce the involvement of the operating system and the CPU usage in the transfer endeavours to almost nothing. In the traditional TCP/IP or Ethernet byte-stream oriented approach all the shared network resources are owned and managed by the operating system - user application does not have direct access to the network. If user application is to transfer data it has to request the operating system to conduct the transfer - the OS is to move data from application's virtual buffer space, down the consecutive layers of the network protocol stack until data are sent into physical medium. On the receiver side the same process take place, but in reversed order - data received from the wire are moved upwards the network stack by the receiver's operating system up to the destination application's virtual buffer space. At all of the stages the OS is involved and CPU time is wasted for multiple copy and move operations. InfiniBand's approach allows to avoid the CPU and OS involvement in data transfer by providing the messaging service to the applications that allows the direct access between the applications' virtual address spaces. It is accomplished by creating channels between the applications that need to communicate, channels' endpoints are called Queue Pairs (QPs), as each of them include a Send Queue and a Receive Queue. The Queue Pair structure provide the application with the access to InfiniBand's messaging service and to avoid the OS involvement QPs are mapped directly into application's virtual address space. It needs to be mentioned that an application may posses more than one Queue Pair and that channels may be established between applications that are in disjoint physical address spaced, i.e. on different servers. This feature is called Remote Direct Memory Access (RDMA) - the messages that are transferred between applications are read/delivered directly from/to application's virtual buffer thus both - the receiving and sending application - are not occupied with the transfer task and the CPUs are not involved - the whole message passing task is accomplished by the IB hardware. It is also important to mention that InfiniBand's messaging service may be as well used for storage and for Inter-Process Communication (IPC) what is essential for HPC systems. The end nodes are connected to the IB network switches via Host Channel Adapters (HCA) - the devices that implement Infiniband's features in hardware, firmware or software. The messaging service is still based on the network stack similar to the one used in traditional networks, but it's reliability is implemented by IB hardware (HCAs and switches).\cite{ibta_IB_whitepaper} What is more the InfiniBand architecture with it's fabric manager - OpenSM - perfectly fits into the concept of Software Defined Networking (SDN) that may became the widely-adopted standard in the upcoming future. \cite{SND_whitepaper, Past_protocols_presentation}
\subsection{InfiniBand history}
The InfiniBand architecture is rooted in the Virtual Interface Architecture (VIA) \cite{via} that was an abstract model of network that provided applications with direct access to a network interface and the possibility to exchange data between theirs virtual buffers without the operating system's help. In 1999 two competing organisations that created implementations of such network - Next Generation I/O (NGIO, including Intel, Sun, Dell and Future I/O (FIO, including IBM, HP, Compaq) \cite{oracle_blog} - were merged and gave rise to InfiniBand Trade Association (IBTA). The IBTA works up to today and is a organisation that consolidate IT vendors in the pursuit of the development of the IB specification. In the early days of the Infiniband a lot of startups sprung up, but almost none of them survived the Dot-Com collapse. One of the survivors was Mellanox Technologies - the world-leading InfiniBand and Ethernet solutions vendor that was acquired by NVIDIA in 2020.\cite{mlnx_timeline} As one may notice, basing on the TOP500 list statistics mentioned before, the InfiniBand technology is still developing, especially in the HPC systems market.
\subsection{Extended Range InfiniBand}
The great majority of InfiniBand networks are spanned within one supercomputer system or one data center - it is rather uncommon to find IB network whose range cover more than one site. Nevertheless, there is a few vendors that offer extended range InfiniBand solutions that allow to interconnect systems on metro and WAN distances. An example of such company is already mentioned Mellanox with it's MetroX solutions that allow to extend RDMA InfiniBand network up to 80 km over dark fiber. \cite{metrox_brief} The Mellanox's TX6240 bundle will be described in detail in the upcoming chapters as it was used for some of the test that are part of this thesis, but at this point it is essential to stress the fact that this system may operate only on dedicated direct fiber link. On the other hand, there are solutions on the market that allow to expand InfiniBand capabilities over global distances and may be tunneled over WAN. Vcinity is an instance of a company that provide such solutions - their Radical X devices that use L2TPv3 tunneling protocol to extend the range of InfiniBand (or lossless Ethernet) over wide area networks. \cite{vcinity_whitepaper} It is worth mentioning that in July 2018 Vcinity acquired Bay Microsystems - a company that developed similar technology before. \cite{vcinity_buy_bay, bay_presentation} Another company that produced this sort of technology was Obsidian Strategics with it's Longbow E100 extenders that allowed to use 10Gb/s InfiniBand over WAN what was state-of-the-art technology in 2014. \cite{longbow} Unfortunately the author was not able to find any recent articles or information on Obsidian Strategics or their new products nor any news on the company's shutdown - it is quite mysterious that it seems to be operational, but did not release any product or major news for the last 6 years. Nevertheless, it is necessary to mention InfiniCortex project that was realised between 2014 and 2016 under the leadership of A*STAR Computational Resource Centre in Singapore and utilised 14 Obsidian Strategics' Longbow E100 and Crossbow R400 router to create an InfiniBand Ring Around the World. The project's aim was to create a single computational resource comprised of a few HPC and storage systems spanned across four continents (Asia, Australia, North America and Europe) that could be used as a single supercomputer system - it could be described with the term "Galaxy of Supercomputers". \cite{infinicortex_KN, infinicloud, infinicortex, infinicortex_SC}
\subsection{RoCE}
While describing InfiniBand and it's RDMA capability it is inevitable to mention the relatively young technology known as RDMA over Converged Ethernet (RoCE, pronounced "rocky") - as the name suggests - it provide RDMA's features in the Ethernet networks. When IEEE 802.1 standard was adopted, that introduced lossless version of Ethernet - Converged Enhanced Ethernet \cite{technopedia_CEE_definition, 802.1_ieee}, it became possible to use widely available technology to achieve RDMA's advantages. However, only initial deployments of RoCE networks required lossless fabric and hardware vendors rapidly came up with solutions that are immune to packet loss and may be deployed on ordinary Ethernet networks. \cite{11_myths_RoCE} The first version of RoCE demanded the host to be in the same broadcast domain (VLAN) in order to communicate. However, this limitation was overcame in RoCE v2 that allowed messages delivery across different subnets, enabling further scalability. \cite{IBM_RoCE_whitepaper} The main superiority of that solution is that it may be used on existing Ethernet infrastructure with the minor cost of RoCE adapters to reduce CPU utilization and bypass the OS for copy and data moving tasks. \cite{mellanox_RoCe_page}
\chapter{Test setup}
During the realisation of the thesis the author conducted a series of the tests to evaluate and compare given network protocols and interconnect technologies - this chapter will describe the hardware that was used and the test preparation.
\section{Hardware specification}
\subsection{Servers}
Two HPE ProLiant DL385 Gen10 Plus servers were used to conduct the tests. HPE lent the servers to ICM UW for evaluation purposes for a few months. The common misconception is that moving enormous amounts of data require only appropriate network with sufficient bandwitdth, but that is only one fraction of necessary resources. To conduct bulk data transfers effeciently over a high-bandwidth network one need powerful server that is able to provide data at the rate that allow to saturate the link efficently. It is impossible to use 100 Gbps network efficiently if the server provide data at 10 Gbps. The crucial aspect is the read/write speed of the storage device that is used in data exchange - the sending side must be able to provide (read) the data at appropriate speed and the receiving side must be able to ingest (write) incoming data. On the first place storage medium is to provide necessary data rate, but on the other hand the server also must posses sufficient computing power and memory to cope with the transmission task. This hardware aspect is frequently neglected by the developers of data transfer solution and they tend to focus on the network bandwidth and software solution. While it is essential to treat the bulk data transfer as a complex issue that need a holistic solution - appropriate servers interconnected with high-bandwidth network and the software that is able to efficiently utilise underlying infrastructure. \cite{Chin_presentation} In an effort to satisfy the mentioned hardware requirements two HPE ProLiant DL385 Gen10 Plus servers, that was customised to provide sufficient storage speed and compute power, were used to conduct the tests. HPE lent the servers to ICM UW for evaluation purposes for a few months - the specification of the servers is listed below:
\begin{itemize}
\item 2 AMD EPYC 7302 16-core (3.0GHz) processors
\item 16 HPE 1x32GB Dual Rank x8 DDR4-3200 CAS-22-22-22 RAM memory (512 GB RAM memory in total)
\item 8 HPE 3.84TB NVMe Gen4 Mainstream Performance Read Intensive SFF SC U.3 CD6 SSD drives
\item 2 240GB SATA SSD drives with HPE E208i-a SR GEN10 12G SAS controller
\item Ethernet 100Gb 2-port QSFP28 MCX516A-CCHT Adapter
\item Intel I350-T4 Ethernet 1Gb 4-port BASE-T OCP3 Adapter for HPE
\item MELLANOX IB ADAPTER - sprawdzić dokładnie jaki i dopisać!
\end{itemize}
ZDJĘCIE SERWERA - zrobić zdjecie tych dwóch serwerów na Kupieckiej i wrzucić tu z podpisem
\subsection{InfiniBand Range Extender}
In the part of the tests Mellanox's MetroX MTX6240 InfiniBand extenders were used to extend IB network between geographically distributed sites over dark fiber. The vendor claims that this system is able to provide 40 Gbps throughput over 40 km of fiber \cite{metrox_brief40} alllowing to benefit from IB features described in previous chapter. MetroX system is implements point-to-point communication - the two MTX6240 bundles, that consist of MEX6240 IB switch and MEX6200 DWDM transponder, are located at the ends of the link - one bundle at each end. The MEX6240 IB switch may be connected to local IB network or directly to IB adapter in the server. During the test conducted for this thesis the MEX6240 switches were connected directly to the IB adapters of HPE ProLiant servers.
ZDJĘCIE METROXA

\bibliography{biblio.bib}
\bibliographystyle{unsrt}


\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:atin-2
%%% End: